{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/infres/pmauduit-21/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/infres/pmauduit-21/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import nltk \n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:2\n",
      "current directory: /home/infres/pmauduit-21/repository/DD2424-DL-Text_Gen_Project\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:2'if torch.cuda.is_available() else \"cpu\") # Use GPU if available\n",
    "print('device:', device)\n",
    "print('current directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 69\n"
     ]
    }
   ],
   "source": [
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "header = alice.find(\"CHAPTER I.\")\n",
    "alice = alice[header:]\n",
    "# raw_text = alice.lower()\n",
    "raw_text = alice\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "n_vocab = len(chars)\n",
    "train_text = raw_text[:-22067]\n",
    "val_text = raw_text[-22067:-11681]\n",
    "test_text = raw_text[-11681:]\n",
    "\n",
    "print('vocab size:', n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim=128, hidden_size=256, num_layers=2, dropout=0, norm=False, type = 'lstm'):\n",
    "        super().__init__()\n",
    "        if embedding_dim > 1:\n",
    "            self.embedding = nn.Embedding(num_embeddings=n_vocab, embedding_dim=embedding_dim)   \n",
    "        if type == 'lstm':\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        elif type == 'gru':\n",
    "            self.lstm = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        elif type == 'rnn':\n",
    "            self.lstm = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if norm:\n",
    "            self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert input to Long type\n",
    "        if hasattr(self, 'embedding'):\n",
    "            x = x.long().squeeze(-1)\n",
    "            x = self.embedding(x)\n",
    "        else:\n",
    "            x = x/float(n_vocab)\n",
    "            if x.dim() == 2:\n",
    "                x.unsqueeze_(-1) # Problem ???\n",
    "        x, _ = self.lstm(x)\n",
    "        # Take only the last output\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0) # Problem with batch size = 1 during inference\n",
    "        x = x[:, -1, :]\n",
    "        # Normalize\n",
    "        if hasattr(self, 'norm'):\n",
    "            x = self.norm(x)\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (embedding): Embedding(69, 100)\n",
       "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=256, out_features=69, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = CharModel(n_vocab, embedding_dim=100, hidden_size=512, num_layers=2, dropout=0.3, norm=True, type='lstm').to(device)\n",
    "model = CharModel(n_vocab, embedding_dim=100, hidden_size=256, num_layers=2, dropout=0.3, norm=True, type='lstm').to(device)\n",
    "seq_length = 100\n",
    "\n",
    "# best_model = torch.load(\"lstm_norm_True_raw_huge.pt\")\n",
    "best_model = torch.load(\"lstm_norm_True_raw.pt\")\n",
    "\n",
    "n_vocab = len(char_to_int)\n",
    "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temperature_scaling(logits, temperature):\n",
    "    logits = logits / temperature\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return probs\n",
    "\n",
    "def nucleus_sampling(probs, p, device):\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)    \n",
    "    cutoff_index = torch.searchsorted(cumulative_probs, torch.tensor(p, device=device)).item() + 1\n",
    "    top_probs = sorted_probs[:cutoff_index]\n",
    "    top_indices = sorted_indices[:cutoff_index]    \n",
    "    top_probs = top_probs / torch.sum(top_probs)    \n",
    "    sampled_index = torch.multinomial(top_probs, 1).item()    \n",
    "    return top_indices[sampled_index].item()\n",
    "\n",
    "def generate_from_text(temperature=None, p = None, prompt=None, doPrint = False):\n",
    "    if prompt is None:\n",
    "        start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "        text_prompt = raw_text[start:start+seq_length]\n",
    "        pattern = [char_to_int[c] for c in text_prompt]\n",
    "        if doPrint:\n",
    "            print('Prompt: \"%s\"' % text_prompt)\n",
    "            print('-'*100)\n",
    "    else:\n",
    "        pattern = [char_to_int[c] for c in prompt]\n",
    "        if doPrint:\n",
    "            print('Prompt: \"%s\"' % prompt)\n",
    "            print('-'*100)\n",
    "    model.eval()\n",
    "    generated_text = \"\"\n",
    "    if prompt is not None:\n",
    "        if doPrint:\n",
    "            print(prompt, end=\"\")\n",
    "        generated_text += prompt\n",
    "    with torch.no_grad():\n",
    "        for i in range(1000):\n",
    "            x = np.reshape(pattern, (1, len(pattern)))\n",
    "            # x = torch.tensor(x, dtype=torch.float32)\n",
    "            x = torch.tensor(x, dtype=torch.long)\n",
    "            logits = model(x.to(device))\n",
    "            if temperature is not None:\n",
    "                scaled_probs = apply_temperature_scaling(logits, temperature)\n",
    "                if p is not None:                \n",
    "                    scaled_probs = scaled_probs.squeeze()\n",
    "                    index = nucleus_sampling(scaled_probs, p, device)\n",
    "                else:\n",
    "                    index = torch.multinomial(scaled_probs, 1).item()\n",
    "            else:\n",
    "                index = int(logits.argmax())\n",
    "            result = int_to_char[index]\n",
    "            generated_text += result\n",
    "            if doPrint:\n",
    "                print(result, end=\"\")\n",
    "            pattern.append(index)\n",
    "            if len(pattern) > seq_length:\n",
    "                pattern = pattern[1:]\n",
    "    if doPrint:\n",
    "        print()\n",
    "    print(\"Done.\")    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TTR score (Type Token Ratio: measure of vocabulary variation)\n",
    "\n",
    "The higher the TTR score, the more diverse the vocabulary used in the text.\n",
    "\n",
    "Temperature sampling gives higher TTR scores than nucleus sampling, but text is less coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ttr(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    ttr = len(unique_words) / len(words)\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Greedy Text TTR: 0.2361111111111111\n",
      "Temperature Text TTR: 0.7\n",
      "Nucleus Text TTR: 0.6089108910891089\n"
     ]
    }
   ],
   "source": [
    "greedy_text = generate_from_text()\n",
    "temperature_text = generate_from_text(temperature=0.8)\n",
    "nucleus_text = generate_from_text(temperature=0.8, p=0.9)\n",
    "\n",
    "print(\"Greedy Text TTR:\", calculate_ttr(greedy_text))\n",
    "print(\"Temperature Text TTR:\", calculate_ttr(temperature_text))\n",
    "print(\"Nucleus Text TTR:\", calculate_ttr(nucleus_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity (how well the model predicts a word sequence)\n",
    "\n",
    "The lower the perplexity, the better the model predicts the next word.\n",
    "\n",
    "Greedy sampling gives the lowest perplexity, but text is also the least diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, model, tokenizer, device):\n",
    "    encodings = tokenizer(text, return_tensors='pt')\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "\n",
    "    lls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone().to(device)\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        lls.append(log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity greedy: 3.982386589050293\n",
      "Perplexity temperature: 112.10043334960938\n",
      "Perplexity nucleus: 98.43819427490234\n",
      "Perplexity reference: 50.75355911254883\n"
     ]
    }
   ],
   "source": [
    "# Reference text for BLEU score calculation\n",
    "reference_text = raw_text[-1000:]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "perplexity_greedy = calculate_perplexity(greedy_text, gpt2_model, tokenizer, device)\n",
    "print(f\"Perplexity greedy: {perplexity_greedy}\")\n",
    "\n",
    "perplexity_temperature = calculate_perplexity(temperature_text, gpt2_model, tokenizer, device)\n",
    "print(f\"Perplexity temperature: {perplexity_temperature}\")\n",
    "\n",
    "perplexity_nucleus = calculate_perplexity(nucleus_text, gpt2_model, tokenizer, device)\n",
    "print(f\"Perplexity nucleus: {perplexity_nucleus}\")\n",
    "\n",
    "perplexity = calculate_perplexity(reference_text, gpt2_model, tokenizer, device)\n",
    "print(f\"Perplexity reference: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spell Check\n",
    "\n",
    "We clean the text by removing special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Text Percentage of correctly spelled words: 99.55%\n",
      "Temperature Text Percentage of correctly spelled words: 94.50%\n",
      "Nucleus Text Percentage of correctly spelled words: 97.66%\n",
      "Reference Text Percentage of correctly spelled words: 99.46%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def calculate_correctly_spelled_percentage(text):\n",
    "    # Initialize the spell checker\n",
    "    spell = SpellChecker()\n",
    "\n",
    "    # Remove punctuation and split the text into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    # Check each word for correctness\n",
    "    misspelled = spell.unknown(words)\n",
    "\n",
    "    # Calculate the number of correctly spelled words\n",
    "    correctly_spelled_count = len(words) - len(misspelled)\n",
    "\n",
    "    # Calculate the percentage of correctly spelled words\n",
    "    if len(words) == 0:\n",
    "        return 0.0  # Avoid division by zero if there are no words\n",
    "    correctly_spelled_percentage = (correctly_spelled_count / len(words)) * 100\n",
    "\n",
    "    return correctly_spelled_percentage\n",
    "\n",
    "greedy_text_percentage = calculate_correctly_spelled_percentage(greedy_text)\n",
    "print(f\"Greedy Text Percentage of correctly spelled words: {greedy_text_percentage:.2f}%\")\n",
    "\n",
    "temperature_text_percentage = calculate_correctly_spelled_percentage(temperature_text)\n",
    "print(f\"Temperature Text Percentage of correctly spelled words: {temperature_text_percentage:.2f}%\")\n",
    "\n",
    "nucleus_text_percentage = calculate_correctly_spelled_percentage(nucleus_text)\n",
    "print(f\"Nucleus Text Percentage of correctly spelled words: {nucleus_text_percentage:.2f}%\")\n",
    "\n",
    "reference_text_percentage = calculate_correctly_spelled_percentage(reference_text)\n",
    "print(f\"Reference Text Percentage of correctly spelled words: {reference_text_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 1.19\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu_score(reference_text, generated_text):\n",
    "    # Tokenize the reference and generated text\n",
    "    reference_tokens = nltk.word_tokenize(reference_text)\n",
    "    generated_tokens = nltk.word_tokenize(generated_text)\n",
    "\n",
    "    # Reference text should be a list of lists of tokens\n",
    "    reference_list = [reference_tokens]\n",
    "\n",
    "    # Use SmoothingFunction to handle very short texts\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = sentence_bleu(reference_list, generated_tokens, smoothing_function=smoothing_function)\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "# Example usage\n",
    "# reference_text = alice[1000:2000]\n",
    "reference_text = alice[-1000:]\n",
    "\n",
    "generated_text = nucleus_text\n",
    "# generated_text = temperature_text\n",
    "# generated_text = greedy_text\n",
    "\n",
    "# generated_text = alice[12000:13000]\n",
    "\n",
    "bleu_score = calculate_bleu_score(reference_text, generated_text) * 100\n",
    "print(f\"Average BLEU Score: {bleu_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
